{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Large Language Models (LLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large Language Models (LLMs) are a type of deep learning model trained on massive amounts of text data. They can understand and generate human-like text, perform various language tasks, and demonstrate emergent capabilities.\n",
    "\n",
    "<span style=\"color : red\">Band 5 & 6 students should understand how LLMs work at a high level, their capabilities and limitations, and ethical considerations around their use.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Makes a Language Model \"Large\"?\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Language Model Size Factors] --> B[Parameters]\n",
    "    A --> C[Training Data]\n",
    "    A --> D[Compute Resources]\n",
    "    \n",
    "    B --> B1[Billions of parameters<br/>7B, 13B, 70B, 175B+]\n",
    "    C --> C1[Terabytes of text<br/>Books, websites, code]\n",
    "    D --> D1[Thousands of GPUs<br/>Millions of dollars]\n",
    "    \n",
    "    style A fill:#e1f5ff,color:#333\n",
    "    style B fill:#d4edda,color:#333\n",
    "    style C fill:#fff3cd,color:#333\n",
    "    style D fill:#ffeaa7,color:#333\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution of Language Models\n",
    "\n",
    "| Era | Model Type | Example | Parameters | Key Innovation |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Early | Statistical | N-grams | N/A | Word frequency patterns |\n",
    "| 2013-2017 | Word Embeddings | Word2Vec, GloVe | Millions | Vector representations |\n",
    "| 2017-2018 | Transformers | Original Transformer | 65M | Self-attention mechanism |\n",
    "| 2018-2019 | Pre-trained LMs | BERT, GPT-2 | 110M-1.5B | Transfer learning |\n",
    "| 2020-2022 | Large LMs | GPT-3, PaLM | 175B-540B | Scale and few-shot learning |\n",
    "| 2023+ | Multimodal LLMs | GPT-4, Gemini, Claude | Unknown | Vision, reasoning, tool use |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How LLMs Work: High-Level Overview\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Input Text:<br/>'The cat sat on the'] --> B[Tokenization:<br/>Split into tokens]\n",
    "    B --> C[Token IDs:<br/>245, 3874, 3332...]\n",
    "    C --> D[Embeddings:<br/>Convert to vectors]\n",
    "    D --> E[Transformer Layers:<br/>Process with attention]\n",
    "    E --> F[Output Probabilities:<br/>Predict next token]\n",
    "    F --> G[Next Token:<br/>'mat']\n",
    "    \n",
    "    style A fill:#e1f5ff,color:#333\n",
    "    style B fill:#fff3cd,color:#333\n",
    "    style C fill:#fff3cd,color:#333\n",
    "    style D fill:#d4edda,color:#333\n",
    "    style E fill:#d4edda,color:#333\n",
    "    style F fill:#ffeaa7,color:#333\n",
    "    style G fill:#f8d7da,color:#333\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts in LLMs\n",
    "\n",
    "### 1. Tokenization\n",
    "\n",
    "Breaking text into smaller units (tokens) that the model can process.\n",
    "\n",
    "| Method | Example | Use Case |\n",
    "| --- | --- | --- |\n",
    "| Character-level | \"hello\" → ['h', 'e', 'l', 'l', 'o'] | Small vocabulary, long sequences |\n",
    "| Word-level | \"hello world\" → ['hello', 'world'] | Simple but large vocabulary |\n",
    "| Subword (BPE) | \"unhappiness\" → ['un', 'happiness'] | Balance between character and word |\n",
    "\n",
    "### 2. Context Window\n",
    "\n",
    "The maximum number of tokens the model can consider at once.\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Early Models<br/>512-1024 tokens]\n",
    "    B[GPT-3<br/>2048-4096 tokens]\n",
    "    C[GPT-4<br/>8K-32K tokens]\n",
    "    D[Claude 2/3<br/>100K-200K tokens]\n",
    "    E[Gemini 1.5<br/>1M+ tokens]\n",
    "    \n",
    "    A --> B --> C --> D --> E\n",
    "    \n",
    "    style A fill:#fff3cd,color:#333\n",
    "    style B fill:#fff3cd,color:#333\n",
    "    style C fill:#d4edda,color:#333\n",
    "    style D fill:#d4edda,color:#333\n",
    "    style E fill:#00b894\n",
    "```\n",
    "\n",
    "**Longer context = Can consider more information but requires more computation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate simple token prediction (conceptual)\n",
    "# Simulate next-token prediction probabilities\n",
    "tokens = ['mat', 'table', 'floor', 'chair', 'sofa']\n",
    "probabilities = [0.45, 0.25, 0.15, 0.10, 0.05]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(tokens, probabilities, color=['red' if p == max(probabilities) else 'blue' for p in probabilities])\n",
    "plt.xlabel('Possible Next Tokens')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('LLM Next-Token Prediction: \"The cat sat on the ___\"')\n",
    "plt.ylim(0, 0.5)\n",
    "for i, (token, prob) in enumerate(zip(tokens, probabilities)):\n",
    "    plt.text(i, prob + 0.01, f'{prob:.2f}', ha='center')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Most likely next token: '{tokens[probabilities.index(max(probabilities))]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Process for LLMs\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Pre-training Phase] --> A1[Collect massive text data<br/>Books, web, code, etc.]\n",
    "    A1 --> A2[Next-token prediction task<br/>Unsupervised learning]\n",
    "    A2 --> A3[Train for weeks/months<br/>on thousands of GPUs]\n",
    "    A3 --> B[Base Model<br/>Can generate text]\n",
    "    \n",
    "    B --> C[Fine-tuning Phase]\n",
    "    C --> C1[Supervised fine-tuning<br/>High-quality examples]\n",
    "    C1 --> C2[RLHF: Reinforcement Learning<br/>from Human Feedback]\n",
    "    C2 --> D[Instruction-tuned Model<br/>Follows instructions better]\n",
    "    \n",
    "    D --> E[Alignment & Safety]\n",
    "    E --> E1[Red-teaming<br/>Test for harmful outputs]\n",
    "    E1 --> E2[Additional safety training]\n",
    "    E2 --> F[Production Model<br/>Ready for deployment]\n",
    "    \n",
    "    style A fill:#e1f5ff,color:#333\n",
    "    style B fill:#fff3cd,color:#333\n",
    "    style C fill:#ffeaa7,color:#333\n",
    "    style D fill:#d4edda,color:#333\n",
    "    style E fill:#f8d7da,color:#333\n",
    "    style F fill:#00b894\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capabilities of Modern LLMs\n",
    "\n",
    "### Core Capabilities\n",
    "\n",
    "| Capability | Description | Example |\n",
    "| --- | --- | --- |\n",
    "| Text Generation | Create fluent, coherent text | Story writing, article generation |\n",
    "| Question Answering | Answer questions based on context | \"What is photosynthesis?\" |\n",
    "| Summarization | Condense long text into key points | Summarize a research paper |\n",
    "| Translation | Convert between languages | English to Spanish |\n",
    "| Code Generation | Write and explain code | \"Write a function to sort a list\" |\n",
    "| Analysis | Extract insights from data | Sentiment analysis, entity extraction |\n",
    "\n",
    "### Emergent Capabilities\n",
    "\n",
    "Abilities that appear in larger models but not smaller ones:\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Small Models<br/>< 1B params] --> B[Basic text generation<br/>Simple Q&A]\n",
    "    C[Medium Models<br/>1-10B params] --> D[Better coherence<br/>Some reasoning]\n",
    "    E[Large Models<br/>10-100B params] --> F[Few-shot learning<br/>Complex reasoning<br/>Chain-of-thought]\n",
    "    G[Very Large Models<br/>100B+ params] --> H[Advanced reasoning<br/>Multi-step problems<br/>Creative tasks]\n",
    "    \n",
    "    style A fill:#fff3cd,color:#333\n",
    "    style C fill:#ffeaa7,color:#333\n",
    "    style E fill:#d4edda,color:#333\n",
    "    style G fill:#00b894\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popular LLM Families\n",
    "\n",
    "### Open-Source LLMs\n",
    "\n",
    "```mermaid\n",
    "mindmap\n",
    "  root((Open-Source<br/>LLMs))\n",
    "    LLaMA Family\n",
    "      Meta's LLaMA 2\n",
    "      LLaMA 3\n",
    "      Variants: Alpaca, Vicuna\n",
    "    Mistral AI\n",
    "      Mistral 7B\n",
    "      Mixtral 8x7B\n",
    "    Google\n",
    "      Gemma\n",
    "      T5, Flan-T5\n",
    "    Community\n",
    "      Falcon\n",
    "      MPT\n",
    "      BLOOM\n",
    "```\n",
    "\n",
    "### Proprietary LLMs\n",
    "\n",
    "| Company | Model | Key Features |\n",
    "| --- | --- | --- |\n",
    "| OpenAI | GPT-4, GPT-4 Turbo | Multimodal, strong reasoning |\n",
    "| Anthropic | Claude 3 (Opus, Sonnet, Haiku) | Long context, safety-focused |\n",
    "| Google | Gemini (Ultra, Pro, Nano) | Multimodal, efficient |\n",
    "| Meta | LLaMA (research) | Open weights, strong performance |\n",
    "| Cohere | Command R+ | Enterprise-focused, RAG-optimized |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use LLMs: Prompting Strategies\n",
    "\n",
    "### 1. Zero-Shot Prompting\n",
    "\n",
    "```\n",
    "Prompt: \"Translate 'Hello, how are you?' to French.\"\n",
    "Response: \"Bonjour, comment allez-vous ?\"\n",
    "```\n",
    "\n",
    "### 2. Few-Shot Prompting\n",
    "\n",
    "```\n",
    "Prompt: \n",
    "\"Classify the sentiment:\n",
    "Text: 'I love this product!' → Positive\n",
    "Text: 'This is terrible.' → Negative\n",
    "Text: 'It's okay, nothing special.' → \"\n",
    "\n",
    "Response: \"Neutral\"\n",
    "```\n",
    "\n",
    "### 3. Chain-of-Thought Prompting\n",
    "\n",
    "```\n",
    "Prompt: \"Solve step-by-step: If a train travels 120 km in 2 hours, \n",
    "what is its speed?\"\n",
    "\n",
    "Response: \n",
    "\"Let's solve this step by step:\n",
    "1. Speed = Distance / Time\n",
    "2. Distance = 120 km\n",
    "3. Time = 2 hours\n",
    "4. Speed = 120 / 2 = 60 km/h\n",
    "Therefore, the train's speed is 60 km/h.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model size vs performance trend\n",
    "# Note: These are approximate, conceptual values\n",
    "model_sizes = [0.1, 0.5, 1, 3, 7, 13, 30, 70, 175, 540]\n",
    "performance = [45, 58, 65, 72, 78, 82, 86, 89, 92, 94]\n",
    "training_cost = [1, 5, 10, 50, 200, 500, 2000, 8000, 50000, 200000]  # in thousands of dollars\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Performance vs model size\n",
    "ax1.plot(model_sizes, performance, 'b-o', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Model Size (Billions of Parameters)')\n",
    "ax1.set_ylabel('Performance Score (%)')\n",
    "ax1.set_title('LLM Performance vs Model Size')\n",
    "ax1.set_xscale('log')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Training cost vs model size\n",
    "ax2.plot(model_sizes, training_cost, 'r-s', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Model Size (Billions of Parameters)')\n",
    "ax2.set_ylabel('Training Cost ($1000s USD)')\n",
    "ax2.set_title('Training Cost vs Model Size')\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: Larger models show diminishing returns in performance but exponential increases in cost.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of LLMs\n",
    "\n",
    "### Technical Limitations\n",
    "\n",
    "| Limitation | Description | Impact |\n",
    "| --- | --- | --- |\n",
    "| Hallucinations | Generate false or nonsensical information | Can provide confident but wrong answers |\n",
    "| Context Length | Limited tokens in context window | Cannot process very long documents |\n",
    "| Recency | Training data has a cutoff date | No knowledge of recent events |\n",
    "| Arithmetic | Struggle with precise calculations | Math errors in complex problems |\n",
    "| Consistency | May give different answers to same question | Unreliable for deterministic tasks |\n",
    "| No True Understanding | Pattern matching, not genuine comprehension | Can fail on novel situations |\n",
    "\n",
    "### Ethical Concerns\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Ethical Concerns with LLMs] --> B[Bias & Fairness]\n",
    "    A --> C[Misinformation]\n",
    "    A --> D[Privacy]\n",
    "    A --> E[Environmental Impact]\n",
    "    A --> F[Job Displacement]\n",
    "    \n",
    "    B --> B1[Training data reflects<br/>societal biases]\n",
    "    C --> C1[Can generate misleading<br/>or false content]\n",
    "    D --> D1[May memorize and leak<br/>training data]\n",
    "    E --> E1[Massive energy consumption<br/>for training]\n",
    "    F --> F1[Automation of knowledge<br/>work tasks]\n",
    "    \n",
    "    style A fill:#e1f5ff,color:#333\n",
    "    style B fill:#f8d7da,color:#333\n",
    "    style C fill:#f8d7da,color:#333\n",
    "    style D fill:#f8d7da,color:#333\n",
    "    style E fill:#f8d7da,color:#333\n",
    "    style F fill:#f8d7da,color:#333\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced LLM Techniques\n",
    "\n",
    "### Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[User Query] --> B[Retrieve Relevant<br/>Documents]\n",
    "    B --> C[Document Database<br/>or Vector Store]\n",
    "    C --> D[Retrieved Context]\n",
    "    D --> E[Combine Query<br/>+ Context]\n",
    "    E --> F[LLM]\n",
    "    F --> G[Generated Response<br/>with Citations]\n",
    "    \n",
    "    style A fill:#e1f5ff,color:#333\n",
    "    style C fill:#fff3cd,color:#333\n",
    "    style D fill:#d4edda,color:#333\n",
    "    style F fill:#d4edda,color:#333\n",
    "    style G fill:#f8d7da,color:#333\n",
    "```\n",
    "\n",
    "**Benefits**: Reduces hallucinations, provides up-to-date information, enables source citations\n",
    "\n",
    "### Fine-tuning\n",
    "\n",
    "Adapting a pre-trained model for specific tasks or domains:\n",
    "\n",
    "- **Full Fine-tuning**: Update all model parameters\n",
    "- **LoRA (Low-Rank Adaptation)**: Update small adapter layers (efficient)\n",
    "- **Prompt Tuning**: Only train prompt embeddings\n",
    "\n",
    "### Tool Use / Function Calling\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[User: What's the weather<br/>in Sydney?] --> B[LLM decides to<br/>call weather tool]\n",
    "    B --> C[Execute:<br/>get_weather 'Sydney']\n",
    "    C --> D[API returns:<br/>22°C, Sunny]\n",
    "    D --> E[LLM generates:<br/>It's 22°C and sunny...]\n",
    "    \n",
    "    style A fill:#e1f5ff,color:#333\n",
    "    style B fill:#fff3cd,color:#333\n",
    "    style C fill:#d4edda,color:#333\n",
    "    style D fill:#ffeaa7,color:#333\n",
    "    style E fill:#f8d7da,color:#333\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future of LLMs\n",
    "\n",
    "### Emerging Trends\n",
    "\n",
    "1. **Multimodal Models**: Processing text, images, audio, video together\n",
    "2. **Smaller, More Efficient Models**: Better performance with fewer parameters\n",
    "3. **Longer Context Windows**: Processing entire books or codebases\n",
    "4. **Improved Reasoning**: Better at mathematics, logic, and multi-step problems\n",
    "5. **Specialized Models**: Domain-specific LLMs (medical, legal, code)\n",
    "6. **On-Device Models**: Running LLMs locally on phones and laptops\n",
    "7. **Better Alignment**: More controllable, safer, less biased models\n",
    "\n",
    "```mermaid\n",
    "timeline\n",
    "    title Evolution of LLM Capabilities\n",
    "    2017 : Transformers introduced\n",
    "         : Attention mechanism\n",
    "    2018-2019 : BERT, GPT-2\n",
    "              : Transfer learning\n",
    "    2020-2022 : GPT-3, Large models\n",
    "              : Few-shot learning\n",
    "    2023 : GPT-4, Claude, Gemini\n",
    "         : Multimodal capabilities\n",
    "    2024-2026 : Better reasoning\n",
    "              : Longer context\n",
    "              : Efficiency improvements\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Considerations for Using LLMs\n",
    "\n",
    "### Choosing an LLM\n",
    "\n",
    "| Factor | Considerations |\n",
    "| --- | --- |\n",
    "| **Cost** | API fees vs self-hosting, usage volume |\n",
    "| **Performance** | Benchmark scores, task-specific evaluation |\n",
    "| **Privacy** | Data retention policies, on-premise options |\n",
    "| **Latency** | Response time requirements |\n",
    "| **Context Length** | How much text needs to be processed |\n",
    "| **Capabilities** | Code, multimodal, specific domains |\n",
    "| **License** | Open-source vs proprietary, commercial use |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Clear Prompts**: Be specific and provide context\n",
    "2. **Validation**: Always verify LLM outputs for critical applications\n",
    "3. **Ethical Use**: Consider bias, fairness, and societal impact\n",
    "4. **Human in the Loop**: Use LLMs to augment, not replace, human judgment\n",
    "5. **Monitoring**: Track performance, costs, and failure modes\n",
    "6. **Privacy**: Don't share sensitive information with external LLM APIs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
