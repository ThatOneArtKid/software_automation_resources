{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "Before we dive into machine learning algorithms, we need to understand what makes machine learning fundamentally different from traditional programming.\n",
    "\n",
    "> **Machine Learning** is a subset of Artificial Intelligence that enables systems to learn and improve from experience without being explicitly programmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Programming vs Machine Learning\n",
    "\n",
    "### Traditional Programming (Algorithmic Approach)\n",
    "\n",
    "In traditional programming, a developer writes explicit rules that transform inputs into outputs. The process is **deterministic** - given the same input, you will always get the same output.\n",
    "\n",
    "```\n",
    "INPUT + RULES → OUTPUT\n",
    "```\n",
    "\n",
    "### Machine Learning Approach\n",
    "\n",
    "In machine learning, we provide data (inputs and expected outputs) and the system learns the rules. The process can be **probabilistic** - the output is based on patterns learned from data.\n",
    "\n",
    "```\n",
    "INPUT + OUTPUT → RULES (Model)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPO Comparison: Traditional Algorithm vs Machine Learning\n",
    "\n",
    "### Example: Adding Two Numbers\n",
    "\n",
    "#### Traditional Algorithm IPO Table\n",
    "\n",
    "| Input | Process | Output |\n",
    "|-------|---------|--------|\n",
    "| Two numbers: 2 and 2 | Apply explicit rule: `return a + b` | 4 (always guaranteed, deterministic) |\n",
    "\n",
    "#### Machine Learning Algorithm IPO Table\n",
    "\n",
    "| Input | Process | Output |\n",
    "|-------|---------|--------|\n",
    "| Two numbers: 2 and 2 | Black box: learned patterns from training data | ~4 (probabilistic, may not be exactly 4) |\n",
    "\n",
    "#### Key Differences\n",
    "\n",
    "| Aspect | Traditional Algorithm | Machine Learning Algorithm |\n",
    "|--------|----------------------|----------------------------|\n",
    "| Transparency | Fully transparent - we wrote the code | Opaque - we don't know exactly how it decided |\n",
    "| Consistency | 100% deterministic | May vary based on model confidence |\n",
    "| Adaptability | Cannot adapt without reprogramming | Can adapt by learning from new data |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional Algorithm - Explicit, Deterministic, Transparent\n",
    "def traditional_add(a, b):\n",
    "    \"\"\"\n",
    "    A simple algorithm with explicit rules.\n",
    "    We know EXACTLY what this function does.\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "# Always returns 4 - guaranteed\n",
    "result = traditional_add(2, 2)\n",
    "print(f\"Traditional Algorithm: 2 + 2 = {result}\")\n",
    "print(f\"We know exactly how we got this answer: by adding a + b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating a \"Black Box\" ML approach\n",
    "# In reality, ML models learn from data, but this demonstrates the concept\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Training data: examples of addition\n",
    "# The model learns the PATTERN, not the explicit rule\n",
    "X_train = np.array([\n",
    "    [1, 1],\n",
    "    [1, 2],\n",
    "    [2, 1],\n",
    "    [3, 3],\n",
    "    [5, 5],\n",
    "    [4, 3],\n",
    "    [10, 5]\n",
    "])\n",
    "y_train = np.array([2, 3, 3, 6, 10, 7, 15])  # The sums\n",
    "\n",
    "# Create and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Ask the ML model: what is 2 + 2?\n",
    "prediction = model.predict([[2, 2]])[0]\n",
    "print(f\"ML Model Prediction: 2 + 2 = {prediction:.4f}\")\n",
    "print(f\"\\nNotice: The answer is close to 4, but not exactly 4!\")\n",
    "print(f\"The model learned a pattern, but we don't know its exact reasoning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Black Box Problem\n",
    "\n",
    "### What is a Black Box?\n",
    "\n",
    "A **black box** is a system where we can see inputs and outputs, but the internal workings are hidden or too complex to understand.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    INPUT --> BB[BLACK BOX<br/>Hidden Logic<br/>? ? ? ? ?]\n",
    "    BB --> OUTPUT\n",
    "    style BB fill:#333,stroke:#fff,color:#fff\n",
    "```\n",
    "\n",
    "### Why Machine Learning is Often a Black Box\n",
    "\n",
    "| Aspect | Traditional Code | Machine Learning Model |\n",
    "|--------|-----------------|------------------------|\n",
    "| **Logic** | Written by humans, readable | Learned from data, mathematical weights |\n",
    "| **Explainability** | Can trace every step | Often cannot explain why a decision was made |\n",
    "| **Debugging** | Step through code line by line | Analyze patterns in millions of parameters |\n",
    "| **Trust** | Trust the programmer | Trust the training data and process |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating the \"black box\" nature\n",
    "print(\"What the ML model actually learned:\")\n",
    "print(f\"Coefficients (weights): {model.coef_}\")\n",
    "print(f\"Intercept (bias): {model.intercept_}\")\n",
    "print(f\"\\nThe model learned: output = {model.coef_[0]:.4f}*a + {model.coef_[1]:.4f}*b + {model.intercept_:.4f}\")\n",
    "print(f\"\\nIdeally it should be: output = 1*a + 1*b + 0\")\n",
    "print(f\"\\nEven for this simple example, the model's internal 'reasoning' is not perfect.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias in Machine Learning\n",
    "\n",
    "### What is Bias?\n",
    "\n",
    "**Bias** in machine learning refers to systematic errors that cause the model to consistently make wrong predictions in a particular direction. Bias can come from:\n",
    "\n",
    "1. **Data Bias** - The training data doesn't represent the real world\n",
    "2. **Algorithm Bias** - The model's assumptions don't match reality\n",
    "3. **Human Bias** - Prejudices of the people who collect data or design systems\n",
    "\n",
    "### Types of Bias\n",
    "\n",
    "| Type | Description | Example |\n",
    "|------|-------------|----------|\n",
    "| **Selection Bias** | Training data not representative | Training facial recognition only on certain demographics |\n",
    "| **Confirmation Bias** | Model reinforces existing patterns | Hiring AI that favors historically hired demographics |\n",
    "| **Historical Bias** | Past data reflects past inequalities | Loan approval models based on historically discriminatory lending |\n",
    "| **Measurement Bias** | Data collection method is flawed | Using arrest records as proxy for criminal behavior |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating Data Bias\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Biased training data - only small numbers!\n",
    "X_biased = np.array([\n",
    "    [1, 1],\n",
    "    [1, 2],\n",
    "    [2, 1],\n",
    "    [2, 2],\n",
    "    [3, 2]\n",
    "])\n",
    "y_biased = np.array([2, 3, 3, 4, 5])\n",
    "\n",
    "biased_model = LinearRegression()\n",
    "biased_model.fit(X_biased, y_biased)\n",
    "\n",
    "# Test on data similar to training\n",
    "print(\"Testing on data SIMILAR to training data:\")\n",
    "print(f\"2 + 2 = {biased_model.predict([[2, 2]])[0]:.2f} (expected: 4)\")\n",
    "\n",
    "# Test on data DIFFERENT from training (larger numbers)\n",
    "print(f\"\\nTesting on data DIFFERENT from training data:\")\n",
    "print(f\"100 + 100 = {biased_model.predict([[100, 100]])[0]:.2f} (expected: 200)\")\n",
    "print(f\"50 + 75 = {biased_model.predict([[50, 75]])[0]:.2f} (expected: 125)\")\n",
    "\n",
    "print(f\"\\n⚠️ The model was only trained on small numbers!\")\n",
    "print(f\"This is DATA BIAS - the training data didn't represent all possible inputs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics in Machine Learning\n",
    "\n",
    "### Why Ethics Matter\n",
    "\n",
    "Machine learning models are increasingly used to make decisions that affect people's lives:\n",
    "- Who gets a loan?\n",
    "- Who gets hired?\n",
    "- Who gets parole?\n",
    "- What medical treatment is recommended?\n",
    "\n",
    "### Key Ethical Considerations\n",
    "\n",
    "| Principle | Description | Question to Ask |\n",
    "|-----------|-------------|------------------|\n",
    "| **Fairness** | Model treats all groups equitably | Does this model disadvantage any group? |\n",
    "| **Transparency** | Decisions can be explained | Can we explain WHY a decision was made? |\n",
    "| **Accountability** | Someone is responsible for outcomes | Who is responsible when the model is wrong? |\n",
    "| **Privacy** | Personal data is protected | What data does this model use and store? |\n",
    "| **Safety** | Model doesn't cause harm | What happens if this model makes a mistake? |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-World Examples of ML Ethics Issues\n",
    "\n",
    "#### 1. Amazon's Hiring Algorithm (2018)\n",
    "Amazon developed an AI recruiting tool that showed bias against women. The model was trained on 10 years of resumes, which were predominantly from men. The system learned to penalize resumes containing words like \"women's\" (as in \"women's chess club captain\").\n",
    "\n",
    "**Lesson:** Historical bias in training data leads to discriminatory models.\n",
    "\n",
    "#### 2. COMPAS Recidivism Algorithm\n",
    "A criminal justice algorithm used in the US to predict the likelihood of reoffending was found to be biased against Black defendants, incorrectly flagging them as higher risk at nearly twice the rate of white defendants.\n",
    "\n",
    "**Lesson:** Black box algorithms making life-changing decisions need transparency and oversight.\n",
    "\n",
    "#### 3. Healthcare Algorithm Bias (2019)\n",
    "A widely-used healthcare algorithm was found to systematically underestimate how sick Black patients were compared to white patients, affecting the care millions received.\n",
    "\n",
    "**Lesson:** Using proxy variables (like healthcare costs) can encode existing inequalities.\n",
    "\n",
    "#### 4. Australian Robodebt Scheme (2016-2019)\n",
    "The Australian Government's \"Robodebt\" scheme used an automated system to calculate welfare debts by averaging annual income data and comparing it against fortnightly payments. The algorithm falsely accused over 400,000 Australians of owing money they did not owe, resulting in wrongful debt notices totaling $1.76 billion. The scheme caused significant financial hardship, mental health crises, and was linked to suicides. A Royal Commission found the scheme was unlawful from its inception.\n",
    "\n",
    "**Lesson:** Automated decision-making systems affecting vulnerable populations require human oversight, transparency, and proper legal review. The \"black box\" nature of the system prevented meaningful appeal processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Machine Learning Pipeline\n",
    "\n",
    "Understanding where bias and ethical issues can enter:\n",
    "\n",
    "| Stage | Process | Potential Bias Issues |\n",
    "|-------|---------|----------------------|\n",
    "| 1. Data Collection | Gather training data | Selection bias, Sampling bias, Historical bias |\n",
    "| 2. Model Selection | Choose algorithm type | Algorithm bias, Wrong assumptions |\n",
    "| 3. Training | Model learns patterns | Overfitting to biased patterns |\n",
    "| 4. Deployment | Model makes predictions | Real-world consequences, Feedback loops |\n",
    "\n",
    "**Pipeline Flow:**\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[DATA COLLECTION] --> B[MODEL SELECTION]\n",
    "    B --> C[TRAINING]\n",
    "    C --> D[DEPLOYMENT]\n",
    "    A -.-> A1[Selection bias]\n",
    "    B -.-> B1[Algorithm bias]\n",
    "    C -.-> C1[Overfitting to bias]\n",
    "    D -.-> D1[Feedback loops]\n",
    "```\n",
    "\n",
    "### Ethical Checkpoints\n",
    "\n",
    "At each stage, we should ask:\n",
    "\n",
    "1. **Data Collection:** Is our data representative? Are we respecting privacy?\n",
    "2. **Model Selection:** Is this model appropriate? Can it be explained?\n",
    "3. **Training:** Are we evaluating for fairness across groups?\n",
    "4. **Deployment:** Who is affected? What happens when it's wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Example: The Danger of Black Box Decision Making\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Simulated \"loan approval\" model - THIS IS A SIMPLIFIED EXAMPLE\n",
    "# In reality, such models are far more complex\n",
    "\n",
    "def black_box_loan_decision(income, credit_score, postcode):\n",
    "    \"\"\"\n",
    "    A simulated black box loan approval system.\n",
    "    Notice: postcode shouldn't affect loan decisions,\n",
    "    but if the training data was biased...\n",
    "    \"\"\"\n",
    "    # Hidden \"learned\" weights (we pretend we don't know these)\n",
    "    score = (income * 0.3) + (credit_score * 0.5) + (postcode * -0.2)\n",
    "    return \"Approved\" if score > 400 else \"Denied\"\n",
    "\n",
    "# Same financial profile, different postcodes\n",
    "print(\"Two applicants with IDENTICAL financial profiles:\")\n",
    "print(f\"Income: $80,000 | Credit Score: 720\")\n",
    "print()\n",
    "print(f\"Applicant A (Postcode 2000): {black_box_loan_decision(80000, 720, 2000)}\")\n",
    "print(f\"Applicant B (Postcode 2770): {black_box_loan_decision(80000, 720, 2770)}\")\n",
    "print()\n",
    "print(\"⚠️ Same qualifications, different outcomes!\")\n",
    "print(\"This is an example of how bias can hide in black box models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Key Concepts\n",
    "\n",
    "### Traditional Programming vs Machine Learning\n",
    "\n",
    "| Traditional | Machine Learning |\n",
    "|-------------|------------------|\n",
    "| Rules are written by humans | Rules are learned from data |\n",
    "| Deterministic outputs | Probabilistic outputs |\n",
    "| Fully transparent | Often a \"black box\" |\n",
    "| Cannot improve without reprogramming | Can improve with more data |\n",
    "\n",
    "### The Black Box\n",
    "- ML models often cannot explain their decisions\n",
    "- This creates challenges for accountability\n",
    "- We need to balance accuracy with interpretability\n",
    "\n",
    "### Bias\n",
    "- Bias can enter at any stage of the ML pipeline\n",
    "- Common sources: data collection, historical patterns, algorithm assumptions\n",
    "- Biased models can perpetuate and amplify existing inequalities\n",
    "\n",
    "### Ethics\n",
    "- ML decisions affect real people's lives\n",
    "- Key principles: fairness, transparency, accountability, privacy, safety\n",
    "- Engineers have a responsibility to consider ethical implications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion Questions\n",
    "\n",
    "1. **Black Box Trade-offs:** Some of the most accurate ML models (like deep neural networks) are also the hardest to explain. When should we prioritize accuracy over explainability? When should we prioritize explainability?\n",
    "\n",
    "2. **Data Responsibility:** If a company trains a model on biased historical data and the model makes unfair decisions, who is responsible - the data scientists, the company, or the people who created the biased data in the first place?\n",
    "\n",
    "3. **Automation vs Human Judgment:** Should some decisions always require human oversight, even if an ML model is more accurate? What types of decisions?\n",
    "\n",
    "4. **Feedback Loops:** If a model is trained on data that includes its own past predictions, how might this create problems? (Hint: think about a recommendation algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension: Exploring Model Interpretability\n",
    "\n",
    "Some ML models are more interpretable than others:\n",
    "\n",
    "| Model Type | Interpretability | Accuracy Potential |\n",
    "|------------|------------------|--------------------|\n",
    "| Linear Regression | High | Lower |\n",
    "| Decision Trees | High | Medium |\n",
    "| Random Forests | Medium | Higher |\n",
    "| Neural Networks | Low | Highest |\n",
    "\n",
    "In subsequent weeks, we'll explore these models and understand the trade-offs between interpretability and performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
