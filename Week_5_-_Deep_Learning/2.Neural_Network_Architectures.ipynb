{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different types of neural network architectures have been developed to solve specific types of problems. This notebook explores the major architectures used in deep learning.\n",
    "\n",
    "<span style=\"color : red\">Band 5 & 6 students know there are different neural network architectures and understand generally their appropriate use cases.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Neural Network Architectures\n",
    "\n",
    "```mermaid\n",
    "mindmap\n",
    "  root((Neural<br/>Networks))\n",
    "    Feedforward\n",
    "      Fully Connected\n",
    "      Deep Neural Networks\n",
    "    Convolutional\n",
    "      Image Classification\n",
    "      Object Detection\n",
    "      Image Segmentation\n",
    "    Recurrent\n",
    "      LSTM\n",
    "      GRU\n",
    "      Time Series\n",
    "    Transformers\n",
    "      Self-Attention\n",
    "      BERT\n",
    "      GPT\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convolutional Neural Networks (CNNs)\n",
    "\n",
    "CNNs are specialized for processing grid-like data, particularly images. They use convolutional layers that apply filters to detect features.\n",
    "\n",
    "### CNN Architecture\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Input Image<br/>32x32x3] --> B[Conv Layer<br/>28x28x32]\n",
    "    B --> C[Pooling<br/>14x14x32]\n",
    "    C --> D[Conv Layer<br/>10x10x64]\n",
    "    D --> E[Pooling<br/>5x5x64]\n",
    "    E --> F[Flatten<br/>1600]\n",
    "    F --> G[Dense Layer<br/>128]\n",
    "    G --> H[Output<br/>10 classes]\n",
    "    \n",
    "    style A fill:#e1f5ff,color:#333\n",
    "    style B fill:#d4edda,color:#333\n",
    "    style C fill:#fff3cd,color:#333\n",
    "    style D fill:#d4edda,color:#333\n",
    "    style E fill:#fff3cd,color:#333\n",
    "    style F fill:#ffeaa7,color:#333\n",
    "    style G fill:#d4edda,color:#333\n",
    "    style H fill:#f8d7da,color:#333\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Components of CNNs\n",
    "\n",
    "| Component | Purpose | Effect on Dimensions |\n",
    "| --- | --- | --- |\n",
    "| Convolutional Layer | Applies filters to detect features (edges, textures, patterns) | Reduces spatial dimensions slightly |\n",
    "| Pooling Layer | Downsamples to reduce computation and prevent overfitting | Reduces spatial dimensions by half |\n",
    "| Activation (ReLU) | Introduces non-linearity | No change in dimensions |\n",
    "| Batch Normalization | Stabilizes and speeds up training | No change in dimensions |\n",
    "| Flatten Layer | Converts 2D feature maps to 1D vector | Converts to 1D |\n",
    "| Dense/Fully Connected | Final classification layers | Reduces to number of classes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Convolution Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate convolution with a simple example\n",
    "# Create a simple 5x5 image\n",
    "image = np.array([\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 1, 0],\n",
    "    [0, 1, 1, 1, 0],\n",
    "    [0, 1, 1, 1, 0],\n",
    "    [0, 0, 0, 0, 0]\n",
    "])\n",
    "\n",
    "# Edge detection filter (vertical edges)\n",
    "filter_vertical = np.array([\n",
    "    [-1, 0, 1],\n",
    "    [-1, 0, 1],\n",
    "    [-1, 0, 1]\n",
    "])\n",
    "\n",
    "# Edge detection filter (horizontal edges)\n",
    "filter_horizontal = np.array([\n",
    "    [-1, -1, -1],\n",
    "    [0, 0, 0],\n",
    "    [1, 1, 1]\n",
    "])\n",
    "\n",
    "def convolve2d(image, kernel):\n",
    "    \"\"\"Simple 2D convolution\"\"\"\n",
    "    output_size = image.shape[0] - kernel.shape[0] + 1\n",
    "    output = np.zeros((output_size, output_size))\n",
    "    \n",
    "    for i in range(output_size):\n",
    "        for j in range(output_size):\n",
    "            output[i, j] = np.sum(image[i:i+3, j:j+3] * kernel)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Apply filters\n",
    "vertical_edges = convolve2d(image, filter_vertical)\n",
    "horizontal_edges = convolve2d(image, filter_horizontal)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(image, cmap='gray')\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(vertical_edges, cmap='gray')\n",
    "axes[1].set_title('Vertical Edge Detection')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(horizontal_edges, cmap='gray')\n",
    "axes[2].set_title('Horizontal Edge Detection')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popular CNN Architectures\n",
    "\n",
    "| Architecture | Year | Key Innovation | Use Case |\n",
    "| --- | --- | --- | --- |\n",
    "| LeNet | 1998 | First successful CNN | Handwriting recognition |\n",
    "| AlexNet | 2012 | Deep CNN with ReLU, Dropout | ImageNet classification |\n",
    "| VGGNet | 2014 | Very deep with small filters | Image classification |\n",
    "| ResNet | 2015 | Skip connections (residual blocks) | Very deep networks (152 layers) |\n",
    "| Inception | 2014 | Multiple filter sizes in parallel | Efficient image classification |\n",
    "| MobileNet | 2017 | Depthwise separable convolutions | Mobile and embedded devices |\n",
    "| EfficientNet | 2019 | Compound scaling | State-of-art efficiency |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recurrent Neural Networks (RNNs)\n",
    "\n",
    "RNNs are designed for sequential data where the order matters, such as time series, text, and speech.\n",
    "\n",
    "### RNN Architecture\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    X1[Input<br/>t=1] --> H1[Hidden<br/>State 1]\n",
    "    H1 --> Y1[Output<br/>t=1]\n",
    "    H1 --> H2[Hidden<br/>State 2]\n",
    "    X2[Input<br/>t=2] --> H2\n",
    "    H2 --> Y2[Output<br/>t=2]\n",
    "    H2 --> H3[Hidden<br/>State 3]\n",
    "    X3[Input<br/>t=3] --> H3\n",
    "    H3 --> Y3[Output<br/>t=3]\n",
    "    H3 --> H4[...]\n",
    "    \n",
    "    style X1 fill:#e1f5ff,color:#333\n",
    "    style X2 fill:#e1f5ff,color:#333\n",
    "    style X3 fill:#e1f5ff,color:#333\n",
    "    style H1 fill:#d4edda,color:#333\n",
    "    style H2 fill:#d4edda,color:#333\n",
    "    style H3 fill:#d4edda,color:#333\n",
    "    style Y1 fill:#f8d7da,color:#333\n",
    "    style Y2 fill:#f8d7da,color:#333\n",
    "    style Y3 fill:#f8d7da,color:#333\n",
    "```\n",
    "\n",
    "**Key Feature**: Hidden state carries information from previous time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Variants\n",
    "\n",
    "#### LSTM (Long Short-Term Memory)\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Previous Cell State] --> B{Forget Gate<br/>What to forget?}\n",
    "    C[Input] --> D{Input Gate<br/>What to add?}\n",
    "    D --> E[New Cell State]\n",
    "    B --> E\n",
    "    E --> F{Output Gate<br/>What to output?}\n",
    "    F --> G[Hidden State]\n",
    "    \n",
    "    style A fill:#e1f5ff,color:#333\n",
    "    style B fill:#fff3cd,color:#333\n",
    "    style C fill:#e1f5ff,color:#333\n",
    "    style D fill:#fff3cd,color:#333\n",
    "    style E fill:#d4edda,color:#333\n",
    "    style F fill:#fff3cd,color:#333\n",
    "    style G fill:#f8d7da,color:#333\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "- Solves vanishing gradient problem\n",
    "- Can learn long-term dependencies\n",
    "- Gates control information flow\n",
    "\n",
    "#### GRU (Gated Recurrent Unit)\n",
    "\n",
    "- Simplified version of LSTM\n",
    "- Fewer parameters, faster training\n",
    "- Combines forget and input gates into update gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Use Cases\n",
    "\n",
    "| Task Type | Example | Architecture |\n",
    "| --- | --- | --- |\n",
    "| One-to-Many | Image captioning | One input → sequence output |\n",
    "| Many-to-One | Sentiment analysis | Sequence input → one output |\n",
    "| Many-to-Many (same length) | Video frame labeling | Sequence → sequence (aligned) |\n",
    "| Many-to-Many (different length) | Machine translation | Sequence → sequence (encoder-decoder) |\n",
    "| Sequence-to-Sequence | Text summarization | Encoder-decoder with attention |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate sequential data processing\n",
    "# Generate a sine wave with noise\n",
    "time_steps = np.linspace(0, 4*np.pi, 100)\n",
    "signal = np.sin(time_steps) + np.random.normal(0, 0.1, 100)\n",
    "\n",
    "# Simulate RNN processing (moving average as simple example)\n",
    "window_size = 5\n",
    "smoothed = np.convolve(signal, np.ones(window_size)/window_size, mode='same')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(time_steps, signal, 'b-', alpha=0.5, label='Noisy Signal (Input)')\n",
    "plt.plot(time_steps, smoothed, 'r-', linewidth=2, label='RNN Processing (Output)')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Value')\n",
    "plt.title('RNN Sequential Processing Example')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformer Architecture\n",
    "\n",
    "Transformers revolutionized NLP and are now used for many tasks. They use self-attention mechanisms instead of recurrence.\n",
    "\n",
    "### Transformer Architecture Overview\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "    A[Input Sequence] --> B[Input Embedding]\n",
    "    B --> C[Positional Encoding]\n",
    "    C --> D[Multi-Head<br/>Self-Attention]\n",
    "    D --> E[Add & Normalize]\n",
    "    E --> F[Feed Forward<br/>Network]\n",
    "    F --> G[Add & Normalize]\n",
    "    G --> H{Repeat N times<br/>Encoder Layers}\n",
    "    H --> I[Encoder Output]\n",
    "    \n",
    "    J[Output Sequence] --> K[Output Embedding]\n",
    "    K --> L[Positional Encoding]\n",
    "    L --> M[Masked Multi-Head<br/>Self-Attention]\n",
    "    M --> N[Add & Normalize]\n",
    "    I --> O[Cross-Attention]\n",
    "    N --> O\n",
    "    O --> P[Add & Normalize]\n",
    "    P --> Q[Feed Forward<br/>Network]\n",
    "    Q --> R[Add & Normalize]\n",
    "    R --> S{Repeat N times<br/>Decoder Layers}\n",
    "    S --> T[Output Probabilities]\n",
    "    \n",
    "    style A fill:#e1f5ff,color:#333\n",
    "    style D fill:#d4edda,color:#333\n",
    "    style I fill:#fff3cd,color:#333\n",
    "    style J fill:#e1f5ff,color:#333\n",
    "    style M fill:#d4edda,color:#333\n",
    "    style O fill:#d4edda,color:#333\n",
    "    style T fill:#f8d7da,color:#333\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Innovations of Transformers\n",
    "\n",
    "#### 1. Self-Attention Mechanism\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[\"Word 1: 'The'\"] --> Q1[Query]\n",
    "    A --> K1[Key]\n",
    "    A --> V1[Value]\n",
    "    \n",
    "    B[\"Word 2: 'cat'\"] --> Q2[Query]\n",
    "    B --> K2[Key]\n",
    "    B --> V2[Value]\n",
    "    \n",
    "    C[\"Word 3: 'sat'\"] --> Q3[Query]\n",
    "    C --> K3[Key]\n",
    "    C --> V3[Value]\n",
    "    \n",
    "    Q1 --> ATT[Attention<br/>Scores]\n",
    "    Q2 --> ATT\n",
    "    Q3 --> ATT\n",
    "    K1 --> ATT\n",
    "    K2 --> ATT\n",
    "    K3 --> ATT\n",
    "    \n",
    "    ATT --> OUT[Weighted<br/>Values]\n",
    "    V1 --> OUT\n",
    "    V2 --> OUT\n",
    "    V3 --> OUT\n",
    "    \n",
    "    style A fill:#e1f5ff,color:#333\n",
    "    style B fill:#e1f5ff,color:#333\n",
    "    style C fill:#e1f5ff,color:#333\n",
    "    style ATT fill:#fff3cd,color:#333\n",
    "    style OUT fill:#f8d7da,color:#333\n",
    "```\n",
    "\n",
    "**Self-attention allows each word to attend to all other words in the sequence simultaneously**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of Transformers\n",
    "\n",
    "| Feature | Benefit |\n",
    "| --- | --- |\n",
    "| Parallel Processing | Much faster training than RNNs |\n",
    "| Long-Range Dependencies | Can relate distant words easily |\n",
    "| No Vanishing Gradients | Direct connections via attention |\n",
    "| Scalability | Can be trained on massive datasets |\n",
    "| Transfer Learning | Pre-trained models work well on many tasks |\n",
    "\n",
    "### Transformer Models\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Transformer Architecture<br/>2017] --> B[Encoder-Only<br/>Models]\n",
    "    A --> C[Decoder-Only<br/>Models]\n",
    "    A --> D[Encoder-Decoder<br/>Models]\n",
    "    \n",
    "    B --> E[BERT<br/>Bidirectional understanding]\n",
    "    B --> F[RoBERTa<br/>Robustly optimized BERT]\n",
    "    \n",
    "    C --> G[GPT Series<br/>Text generation]\n",
    "    C --> H[LLaMA<br/>Open-source LLM]\n",
    "    \n",
    "    D --> I[T5<br/>Text-to-text]\n",
    "    D --> J[BART<br/>Denoising autoencoder]\n",
    "    \n",
    "    style A fill:#e1f5ff,color:#333\n",
    "    style B fill:#d4edda,color:#333\n",
    "    style C fill:#fff3cd,color:#333\n",
    "    style D fill:#ffeaa7,color:#333\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Architectures\n",
    "\n",
    "| Architecture | Best For | Strengths | Limitations |\n",
    "| --- | --- | --- | --- |\n",
    "| **CNN** | Images, spatial data | Parameter sharing, translation invariance | Requires fixed-size input |\n",
    "| **RNN/LSTM** | Sequential data, time series | Handles variable length sequences | Slow to train, vanishing gradients |\n",
    "| **Transformer** | NLP, any sequential data | Parallel processing, long-range dependencies | High memory requirements |\n",
    "| **Hybrid** | Complex tasks | Combines strengths of multiple architectures | More complex to implement |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Architectures\n",
    "\n",
    "Modern deep learning often combines different architectures:\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Image] --> B[CNN<br/>Feature Extraction]\n",
    "    B --> C[Sequence of<br/>Features]\n",
    "    C --> D[Transformer<br/>or RNN]\n",
    "    D --> E[Caption<br/>or Description]\n",
    "    \n",
    "    style A fill:#e1f5ff,color:#333\n",
    "    style B fill:#d4edda,color:#333\n",
    "    style C fill:#fff3cd,color:#333\n",
    "    style D fill:#d4edda,color:#333\n",
    "    style E fill:#f8d7da,color:#333\n",
    "```\n",
    "\n",
    "**Examples**:\n",
    "- **Vision Transformers (ViT)**: Treat image patches as sequences\n",
    "- **CLIP**: Combines vision and language understanding\n",
    "- **Video Understanding**: CNN + Transformer for spatial-temporal analysis\n",
    "- **Speech Recognition**: CNN for features + Transformer for sequence modeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
