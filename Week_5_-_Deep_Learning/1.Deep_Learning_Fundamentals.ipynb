{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning is a subset of machine learning that uses artificial neural networks with multiple layers (deep neural networks) to learn hierarchical representations of data. Unlike traditional machine learning algorithms that require manual feature engineering, deep learning models automatically learn relevant features from raw data.\n",
    "\n",
    "<span style=\"color : red\">Band 5 & 6 students should understand the difference between traditional machine learning and deep learning, and be able to identify appropriate use cases for deep learning.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Makes Deep Learning Different?\n",
    "\n",
    "### Traditional Machine Learning vs Deep Learning\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Raw Data] --> B[Feature Engineering]\n",
    "    B --> C[ML Algorithm]\n",
    "    C --> D[Output]\n",
    "    \n",
    "    style A fill:#e1f5ff,color:#333\n",
    "    style B fill:#fff3cd,color:#333\n",
    "    style C fill:#d4edda,color:#333\n",
    "    style D fill:#f8d7da,color:#333\n",
    "```\n",
    "\n",
    "**Traditional ML:** Requires manual feature engineering\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Raw Data] --> B[Deep Neural Network]\n",
    "    B --> C[Learned Features]\n",
    "    C --> D[Output]\n",
    "    \n",
    "    style A fill:#e1f5ff,color:#333\n",
    "    style B fill:#d4edda,color:#333\n",
    "    style C fill:#d4edda,color:#333\n",
    "    style D fill:#f8d7da,color:#333\n",
    "```\n",
    "\n",
    "**Deep Learning:** Automatically learns features from data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts in Deep Learning\n",
    "\n",
    "| Term | Definition |\n",
    "| --- | --- |\n",
    "| Deep Neural Network | A neural network with multiple hidden layers (typically 3+) |\n",
    "| Feature Learning | Automatic extraction of useful features from raw data |\n",
    "| Hierarchical Representation | Learning features at multiple levels of abstraction |\n",
    "| Backpropagation | Algorithm for training deep networks by propagating errors backward |\n",
    "| Activation Function | Non-linear function that introduces complexity (ReLU, Sigmoid, Tanh) |\n",
    "| Dropout | Regularization technique to prevent overfitting |\n",
    "| Batch Normalization | Technique to stabilize and speed up training |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network Architecture\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    I1((Input 1)) --> H11((H1))\n",
    "    I2((Input 2)) --> H11\n",
    "    I3((Input 3)) --> H11\n",
    "    I1 --> H12((H2))\n",
    "    I2 --> H12\n",
    "    I3 --> H12\n",
    "    I1 --> H13((H3))\n",
    "    I2 --> H13\n",
    "    I3 --> H13\n",
    "    I1 --> H14((H4))\n",
    "    I2 --> H14\n",
    "    I3 --> H14\n",
    "    \n",
    "    H11 --> H21((H1))\n",
    "    H12 --> H21\n",
    "    H13 --> H21\n",
    "    H14 --> H21\n",
    "    H11 --> H22((H2))\n",
    "    H12 --> H22\n",
    "    H13 --> H22\n",
    "    H14 --> H22\n",
    "    H11 --> H23((H3))\n",
    "    H12 --> H23\n",
    "    H13 --> H23\n",
    "    H14 --> H23\n",
    "    \n",
    "    H21 --> H31((H1))\n",
    "    H22 --> H31\n",
    "    H23 --> H31\n",
    "    H21 --> H32((H2))\n",
    "    H22 --> H32\n",
    "    H23 --> H32\n",
    "    \n",
    "    H31 --> O1((Output 1))\n",
    "    H32 --> O1\n",
    "    H31 --> O2((Output 2))\n",
    "    H32 --> O2\n",
    "    \n",
    "    style I1 fill:#e1f5ff,color:#333\n",
    "    style I2 fill:#e1f5ff,color:#333\n",
    "    style I3 fill:#e1f5ff,color:#333\n",
    "    style O1 fill:#f8d7da,color:#333\n",
    "    style O2 fill:#f8d7da,color:#333\n",
    "```\n",
    "\n",
    "**Input Layer → Hidden Layer 1 → Hidden Layer 2 → Hidden Layer 3 → Output Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions in Deep Learning\n",
    "\n",
    "Activation functions introduce non-linearity, allowing neural networks to learn complex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common activation functions\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "# ReLU (Rectified Linear Unit) - most common in deep learning\n",
    "relu = np.maximum(0, x)\n",
    "\n",
    "# Sigmoid\n",
    "sigmoid = 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Tanh\n",
    "tanh = np.tanh(x)\n",
    "\n",
    "# Leaky ReLU\n",
    "leaky_relu = np.where(x > 0, x, x * 0.01)\n",
    "\n",
    "# Plot all activation functions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "axes[0, 0].plot(x, relu, 'r-', linewidth=2)\n",
    "axes[0, 0].set_title('ReLU Activation')\n",
    "axes[0, 0].set_xlabel('Input')\n",
    "axes[0, 0].set_ylabel('Output')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "axes[0, 1].plot(x, sigmoid, 'b-', linewidth=2)\n",
    "axes[0, 1].set_title('Sigmoid Activation')\n",
    "axes[0, 1].set_xlabel('Input')\n",
    "axes[0, 1].set_ylabel('Output')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "axes[1, 0].plot(x, tanh, 'g-', linewidth=2)\n",
    "axes[1, 0].set_title('Tanh Activation')\n",
    "axes[1, 0].set_xlabel('Input')\n",
    "axes[1, 0].set_ylabel('Output')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "axes[1, 1].plot(x, leaky_relu, 'm-', linewidth=2)\n",
    "axes[1, 1].set_title('Leaky ReLU Activation')\n",
    "axes[1, 1].set_xlabel('Input')\n",
    "axes[1, 1].set_ylabel('Output')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Activation Functions\n",
    "\n",
    "| Activation | Use Case | Advantages | Disadvantages |\n",
    "| --- | --- | --- | --- |\n",
    "| ReLU | Hidden layers in most networks | Fast computation, no vanishing gradient for positive values | Dead neurons (outputs 0 for negative inputs) |\n",
    "| Leaky ReLU | When ReLU causes dead neurons | Prevents dying ReLU problem | Slight increase in computation |\n",
    "| Sigmoid | Binary classification output layer | Output between 0-1 (probability) | Vanishing gradient problem |\n",
    "| Tanh | Hidden layers when centered data needed | Output between -1 to 1, zero-centered | Vanishing gradient problem |\n",
    "| Softmax | Multi-class classification output | Outputs sum to 1 (probability distribution) | Only for output layer |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Deep Learning Requires More Data\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Small Dataset<br/>100-1000 samples] --> B[Traditional ML<br/>Decision Trees, KNN, Linear Models]\n",
    "    C[Medium Dataset<br/>1000-100,000 samples] --> D[Shallow Neural Networks<br/>1-2 hidden layers]\n",
    "    E[Large Dataset<br/>100,000+ samples] --> F[Deep Neural Networks<br/>Many hidden layers]\n",
    "    G[Massive Dataset<br/>Millions of samples] --> H[Very Deep Networks<br/>Transfer Learning, LLMs]\n",
    "    \n",
    "    style A fill:#fff3cd,color:#333\n",
    "    style B fill:#d4edda,color:#333\n",
    "    style C fill:#fff3cd,color:#333\n",
    "    style D fill:#d4edda,color:#333\n",
    "    style E fill:#fff3cd,color:#333\n",
    "    style F fill:#d4edda,color:#333\n",
    "    style G fill:#fff3cd,color:#333\n",
    "    style H fill:#d4edda,color:#333\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Performance vs Traditional ML\n",
    "\n",
    "Deep learning models typically require more data to outperform traditional ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate performance curves\n",
    "data_size = np.logspace(2, 6, 100)  # 100 to 1,000,000 samples\n",
    "\n",
    "# Traditional ML plateaus early\n",
    "traditional_ml = 70 + 25 * (1 - np.exp(-data_size/10000))\n",
    "\n",
    "# Deep learning continues to improve with more data\n",
    "deep_learning = 50 + 48 * (1 - np.exp(-data_size/100000))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(data_size, traditional_ml, 'b-', linewidth=2, label='Traditional ML')\n",
    "plt.semilogx(data_size, deep_learning, 'r-', linewidth=2, label='Deep Learning')\n",
    "plt.xlabel('Amount of Training Data (samples)')\n",
    "plt.ylabel('Model Performance (%)')\n",
    "plt.title('Performance vs Data Size: Traditional ML vs Deep Learning')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axvline(x=10000, color='green', linestyle='--', alpha=0.5, label='Crossover Point')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Deep Learning Challenges\n",
    "\n",
    "### 1. Vanishing Gradient Problem\n",
    "\n",
    "In very deep networks, gradients can become extremely small as they're propagated backwards, making it difficult to train early layers.\n",
    "\n",
    "**Solutions:**\n",
    "- Use ReLU activation functions\n",
    "- Implement batch normalization\n",
    "- Use residual connections (ResNet)\n",
    "- Careful weight initialization\n",
    "\n",
    "### 2. Overfitting\n",
    "\n",
    "Deep networks have millions of parameters and can easily memorize training data.\n",
    "\n",
    "**Solutions:**\n",
    "- Dropout regularization\n",
    "- Data augmentation\n",
    "- Early stopping\n",
    "- L1/L2 regularization\n",
    "\n",
    "### 3. Computational Requirements\n",
    "\n",
    "Training deep networks requires significant computational resources.\n",
    "\n",
    "**Solutions:**\n",
    "- Use GPUs/TPUs\n",
    "- Transfer learning (use pre-trained models)\n",
    "- Model compression\n",
    "- Efficient architectures (MobileNet, EfficientNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Process Visualization\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Initialize Network<br/>Random Weights] --> B[Forward Pass<br/>Make Predictions]\n",
    "    B --> C[Calculate Loss<br/>Compare to True Values]\n",
    "    C --> D[Backward Pass<br/>Compute Gradients]\n",
    "    D --> E[Update Weights<br/>Using Optimizer]\n",
    "    E --> F{Converged?}\n",
    "    F -->|No| B\n",
    "    F -->|Yes| G[Final Model]\n",
    "    \n",
    "    style A fill:#e1f5ff,color:#333\n",
    "    style B fill:#fff3cd,color:#333\n",
    "    style C fill:#f8d7da,color:#333\n",
    "    style D fill:#fff3cd,color:#333\n",
    "    style E fill:#d4edda,color:#333\n",
    "    style F fill:#ffeaa7,color:#333\n",
    "    style G fill:#00b894\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use Deep Learning\n",
    "\n",
    "### Good Use Cases:\n",
    "- **Image Recognition**: Object detection, facial recognition, medical imaging\n",
    "- **Natural Language Processing**: Translation, sentiment analysis, text generation\n",
    "- **Speech Recognition**: Voice assistants, transcription services\n",
    "- **Complex Pattern Recognition**: Where features are difficult to engineer manually\n",
    "- **Large datasets available**: Millions of training examples\n",
    "\n",
    "### When Traditional ML May Be Better:\n",
    "- Small datasets (< 10,000 samples)\n",
    "- Simple, well-defined problems\n",
    "- Need for interpretability\n",
    "- Limited computational resources\n",
    "- Fast training time required\n",
    "- Structured/tabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Frameworks\n",
    "\n",
    "| Framework | Organization | Strengths |\n",
    "| --- | --- | --- |\n",
    "| TensorFlow | Google | Production deployment, TensorFlow Lite for mobile |\n",
    "| PyTorch | Meta (Facebook) | Research-friendly, dynamic computation graphs |\n",
    "| Keras | Google | High-level API, beginner-friendly |\n",
    "| JAX | Google | High-performance numerical computing |\n",
    "| MXNet | Apache | Scalable, efficient |\n",
    "\n",
    "**Note**: Keras is now integrated into TensorFlow as `tf.keras` and is the recommended high-level API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
