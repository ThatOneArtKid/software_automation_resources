{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Responsible AI and Ethics in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As deep learning systems become more powerful and widely deployed, understanding their ethical implications and ensuring responsible development is crucial.\n",
    "\n",
    "<span style=\"color : red\">Band 5 & 6 students should be able to identify ethical concerns in AI systems, understand fairness and bias issues, and recognize the importance of responsible AI development.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Ethical Principles in AI\n",
    "\n",
    "```mermaid\n",
    "mindmap\n",
    "  root((Responsible<br/>AI))\n",
    "    Fairness\n",
    "      Bias Mitigation\n",
    "      Equal Treatment\n",
    "      Representation\n",
    "    Transparency\n",
    "      Explainability\n",
    "      Documentation\n",
    "      Open Communication\n",
    "    Privacy\n",
    "      Data Protection\n",
    "      Consent\n",
    "      Anonymization\n",
    "    Accountability\n",
    "      Human Oversight\n",
    "      Responsibility\n",
    "      Recourse\n",
    "    Safety\n",
    "      Robustness\n",
    "      Security\n",
    "      Reliability\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding AI Bias\n",
    "\n",
    "### Types of Bias in ML Systems\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Sources of Bias] --> B[Historical Bias]\n",
    "    A --> C[Representation Bias]\n",
    "    A --> D[Measurement Bias]\n",
    "    A --> E[Aggregation Bias]\n",
    "    A --> F[Evaluation Bias]\n",
    "    \n",
    "    B --> B1[Past societal biases<br/>reflected in data]\n",
    "    C --> C1[Underrepresentation of<br/>certain groups]\n",
    "    D --> D1[Inconsistent or biased<br/>measurement methods]\n",
    "    E --> E1[Model assumes one size<br/>fits all]\n",
    "    F --> F1[Testing on non-representative<br/>populations]\n",
    "    \n",
    "    style A fill:#e1f5ff,color:#333\n",
    "    style B fill:#f8d7da,color:#333\n",
    "    style C fill:#f8d7da,color:#333\n",
    "    style D fill:#f8d7da,color:#333\n",
    "    style E fill:#f8d7da,color:#333\n",
    "    style F fill:#f8d7da,color:#333\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Examples of AI Bias\n",
    "\n",
    "| Domain | Issue | Impact |\n",
    "| --- | --- | --- |\n",
    "| **Facial Recognition** | Lower accuracy for people with darker skin tones | Misidentification, false arrests |\n",
    "| **Hiring Tools** | Bias against female candidates in technical roles | Discrimination in employment |\n",
    "| **Criminal Justice** | Higher recidivism risk scores for minorities | Unfair sentencing, parole decisions |\n",
    "| **Healthcare** | Underdiagnosis in underrepresented groups | Health disparities |\n",
    "| **Language Models** | Stereotypical associations (gender, race) | Perpetuating harmful stereotypes |\n",
    "| **Credit Scoring** | Bias against certain demographics | Financial discrimination |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate representation bias\n",
    "# Simulate a biased dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "# Group A: Well-represented (1000 samples)\n",
    "group_a_features = np.random.normal(5, 2, 1000)\n",
    "group_a_labels = (group_a_features > 5).astype(int)\n",
    "\n",
    "# Group B: Underrepresented (100 samples)\n",
    "group_b_features = np.random.normal(5, 2, 100)\n",
    "group_b_labels = (group_b_features > 5).astype(int)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Distribution comparison\n",
    "ax1.hist(group_a_features, bins=30, alpha=0.7, label='Group A (1000 samples)', color='blue')\n",
    "ax1.hist(group_b_features, bins=30, alpha=0.7, label='Group B (100 samples)', color='red')\n",
    "ax1.set_xlabel('Feature Value')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Representation Bias in Training Data')\n",
    "ax1.legend()\n",
    "\n",
    "# Sample size comparison\n",
    "groups = ['Group A\\n(Majority)', 'Group B\\n(Minority)']\n",
    "sizes = [1000, 100]\n",
    "colors = ['blue', 'red']\n",
    "ax2.bar(groups, sizes, color=colors, alpha=0.7)\n",
    "ax2.set_ylabel('Number of Samples')\n",
    "ax2.set_title('Sample Size Imbalance')\n",
    "for i, v in enumerate(sizes):\n",
    "    ax2.text(i, v + 20, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Warning: Models trained on imbalanced data may perform worse on underrepresented groups.\")\n",
    "print(\"This is representation bias - a common source of unfairness in ML systems.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness Definitions\n",
    "\n",
    "Different mathematical definitions of fairness:\n",
    "\n",
    "| Fairness Type | Definition | Example |\n",
    "| --- | --- | --- |\n",
    "| **Demographic Parity** | Equal positive outcome rates across groups | Loan approval rate same for all demographics |\n",
    "| **Equal Opportunity** | Equal true positive rates across groups | Disease detection equally sensitive for all groups |\n",
    "| **Equalized Odds** | Equal true positive AND false positive rates | Fair for both groups regardless of outcome |\n",
    "| **Predictive Parity** | Equal precision across groups | When model says \"yes\", equally likely to be correct |\n",
    "| **Individual Fairness** | Similar individuals get similar outcomes | Two similar resumes get similar scores |\n",
    "\n",
    "**Important**: These definitions can be mutually exclusive - achieving one may make others impossible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias Mitigation Strategies\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Bias Mitigation Strategies] --> B[Pre-processing]\n",
    "    A --> C[In-processing]\n",
    "    A --> D[Post-processing]\n",
    "    \n",
    "    B --> B1[Reweighting training data]\n",
    "    B --> B2[Data augmentation for<br/>underrepresented groups]\n",
    "    B --> B3[Remove sensitive attributes]\n",
    "    \n",
    "    C --> C1[Fairness-aware training<br/>objectives]\n",
    "    C --> C2[Adversarial debiasing]\n",
    "    C --> C3[Constraint-based methods]\n",
    "    \n",
    "    D --> D1[Adjust decision thresholds<br/>per group]\n",
    "    D --> D2[Calibrate predictions]\n",
    "    D --> D3[Reject discriminatory outputs]\n",
    "    \n",
    "    style A fill:#e1f5ff,color:#333\n",
    "    style B fill:#fff3cd,color:#333\n",
    "    style C fill:#d4edda,color:#333\n",
    "    style D fill:#ffeaa7,color:#333\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability and Transparency\n",
    "\n",
    "### The Black Box Problem\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Input Data] --> B[???<br/>Deep Neural Network<br/>Millions of Parameters<br/>???]\n",
    "    B --> C[Prediction]\n",
    "    \n",
    "    D[Why this prediction?<br/>Which features mattered?<br/>Can we trust it?] -.-> B\n",
    "    \n",
    "    style A fill:#e1f5ff,color:#333\n",
    "    style B fill:#2d3436\n",
    "    style C fill:#f8d7da,color:#333\n",
    "    style D fill:#fdcb6e,color:#333\n",
    "```\n",
    "\n",
    "### Explainability Techniques\n",
    "\n",
    "| Technique | Type | Use Case |\n",
    "| --- | --- | --- |\n",
    "| **Feature Importance** | Global | Which features matter most overall? |\n",
    "| **SHAP Values** | Local & Global | Explain individual predictions |\n",
    "| **LIME** | Local | Explain specific predictions |\n",
    "| **Attention Visualization** | Model-specific | What does the model focus on? |\n",
    "| **Saliency Maps** | Vision | Which pixels influenced the decision? |\n",
    "| **Counterfactual Explanations** | Local | What would change the prediction? |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate simple feature importance visualization\n",
    "features = ['Age', 'Income', 'Credit\\nScore', 'Employment\\nLength', 'Debt\\nRatio', 'Education']\n",
    "importance = np.array([0.15, 0.35, 0.25, 0.12, 0.08, 0.05])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = plt.cm.viridis(importance / importance.max())\n",
    "bars = plt.barh(features, importance, color=colors)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Model Feature Importance for Loan Approval\\n(Explainability Example)')\n",
    "plt.xlim(0, 0.4)\n",
    "\n",
    "# Add value labels\n",
    "for i, (feat, imp) in enumerate(zip(features, importance)):\n",
    "    plt.text(imp + 0.01, i, f'{imp:.2f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Feature importance helps explain which factors influence model decisions.\")\n",
    "print(\"This transparency is crucial for building trust and identifying potential biases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy Concerns in Deep Learning\n",
    "\n",
    "### Privacy Risks\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Privacy Risks] --> B[Training Data Memorization]\n",
    "    A --> C[Model Inversion Attacks]\n",
    "    A --> D[Membership Inference]\n",
    "    A --> E[Data Leakage]\n",
    "    \n",
    "    B --> B1[Models memorize and can<br/>reproduce training data]\n",
    "    C --> C1[Reconstruct training samples<br/>from model weights]\n",
    "    D --> D1[Determine if data was in<br/>training set]\n",
    "    E --> E1[Sensitive info in prompts<br/>or responses]\n",
    "    \n",
    "    style A fill:#e1f5ff,color:#333\n",
    "    style B fill:#f8d7da,color:#333\n",
    "    style C fill:#f8d7da,color:#333\n",
    "    style D fill:#f8d7da,color:#333\n",
    "    style E fill:#f8d7da,color:#333\n",
    "```\n",
    "\n",
    "### Privacy-Preserving Techniques\n",
    "\n",
    "| Technique | How It Works | Trade-off |\n",
    "| --- | --- | --- |\n",
    "| **Differential Privacy** | Add noise to data/gradients | Slight accuracy decrease |\n",
    "| **Federated Learning** | Train on decentralized data | Communication overhead |\n",
    "| **Homomorphic Encryption** | Compute on encrypted data | Very slow |\n",
    "| **Secure Multi-party Computation** | Multiple parties compute without sharing | Complexity |\n",
    "| **Data Anonymization** | Remove identifying information | May not prevent re-identification |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environmental Impact of AI\n",
    "\n",
    "### Carbon Footprint of Training Large Models\n",
    "\n",
    "Training large deep learning models has significant environmental costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate CO2 emissions for training various models\n",
    "models = ['BERT-base', 'GPT-2', 'GPT-3', 'Large Vision\\nModel', 'Your ML\\nProject']\n",
    "co2_tons = [0.07, 1.5, 550, 300, 0.001]  # Approximate CO2 emissions in metric tons\n",
    "equivalents = ['7 hours driving', '2 weeks driving', '5 car lifetimes', '3 car lifetimes', '1 minute driving']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "colors = ['green', 'yellow', 'red', 'orange', 'blue']\n",
    "bars = ax.bar(models, co2_tons, color=colors, alpha=0.7)\n",
    "ax.set_ylabel('CO‚ÇÇ Emissions (metric tons)')\n",
    "ax.set_title('Environmental Impact: CO‚ÇÇ Emissions from Training AI Models')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Add labels\n",
    "for i, (model, emission, equiv) in enumerate(zip(models, co2_tons, equivalents)):\n",
    "    ax.text(i, emission * 1.5, f'{emission} tons\\n‚âà {equiv}', ha='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüåç Consider the environmental impact when training large models.\")\n",
    "print(\"Use transfer learning, efficient architectures, and carbon-aware training when possible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Safety and Robustness\n",
    "\n",
    "### Adversarial Attacks\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Original Image<br/>Panda<br/>Prediction: Panda 99%] --> B[+ Imperceptible<br/>Noise]\n",
    "    B --> C[Adversarial Image<br/>Looks like Panda<br/>Prediction: Gibbon 99%]\n",
    "    \n",
    "    style A fill:#d4edda,color:#333\n",
    "    style B fill:#fff3cd,color:#333\n",
    "    style C fill:#f8d7da,color:#333\n",
    "```\n",
    "\n",
    "### Types of Attacks\n",
    "\n",
    "| Attack Type | Description | Defense |\n",
    "| --- | --- | --- |\n",
    "| **Adversarial Examples** | Carefully crafted inputs to fool model | Adversarial training, input validation |\n",
    "| **Data Poisoning** | Corrupt training data | Data validation, anomaly detection |\n",
    "| **Model Extraction** | Steal model by querying it | Rate limiting, output perturbation |\n",
    "| **Backdoor Attacks** | Hidden triggers in model | Model auditing, clean training |\n",
    "| **Prompt Injection** | Manipulate LLMs with malicious prompts | Input sanitization, safety layers |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Responsible AI Development Framework\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Problem Definition] --> A1[Consider ethical implications<br/>upfront]\n",
    "    A1 --> B[Data Collection]\n",
    "    B --> B1[Ensure diverse, representative data<br/>Protect privacy]\n",
    "    B1 --> C[Model Development]\n",
    "    C --> C1[Build in fairness constraints<br/>Document decisions]\n",
    "    C1 --> D[Testing & Validation]\n",
    "    D --> D1[Test on diverse populations<br/>Evaluate for bias<br/>Red-teaming]\n",
    "    D1 --> E[Deployment]\n",
    "    E --> E1[Human oversight<br/>Monitoring systems<br/>Feedback loops]\n",
    "    E1 --> F[Ongoing Maintenance]\n",
    "    F --> F1[Regular audits<br/>Update as needed<br/>Address issues quickly]\n",
    "    \n",
    "    style A1 fill:#d4edda,color:#333\n",
    "    style B1 fill:#d4edda,color:#333\n",
    "    style C1 fill:#d4edda,color:#333\n",
    "    style D1 fill:#d4edda,color:#333\n",
    "    style E1 fill:#d4edda,color:#333\n",
    "    style F1 fill:#d4edda,color:#333\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Documentation: Model Cards\n",
    "\n",
    "A Model Card documents key information about ML models:\n",
    "\n",
    "### Essential Components\n",
    "\n",
    "1. **Model Details**: Architecture, version, training date\n",
    "2. **Intended Use**: Primary applications, out-of-scope uses\n",
    "3. **Training Data**: Source, size, demographics, limitations\n",
    "4. **Performance**: Metrics across different subgroups\n",
    "5. **Limitations**: Known failures, biases, edge cases\n",
    "6. **Ethical Considerations**: Potential harms, mitigation strategies\n",
    "7. **Recommendations**: Best practices for use\n",
    "\n",
    "**Example**: Google's Model Card Toolkit, Hugging Face model documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regulatory Landscape\n",
    "\n",
    "### Global AI Regulations\n",
    "\n",
    "| Region | Regulation | Key Requirements |\n",
    "| --- | --- | --- |\n",
    "| **EU** | AI Act | Risk-based approach, banned applications |\n",
    "| **Australia** | Privacy Act, voluntary guidelines | Data protection, ethical AI principles |\n",
    "| **USA** | NIST AI RMF, sector-specific rules | Risk management, transparency |\n",
    "| **Canada** | AIDA (proposed) | Impact assessments, accountability |\n",
    "| **Global** | OECD AI Principles | International standards |\n",
    "\n",
    "### High-Risk Applications\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[High-Risk AI Systems] --> B[Healthcare]\n",
    "    A --> C[Criminal Justice]\n",
    "    A --> D[Employment]\n",
    "    A --> E[Education]\n",
    "    A --> F[Critical Infrastructure]\n",
    "    \n",
    "    B --> B1[Require strict oversight<br/>and documentation]\n",
    "    C --> B1\n",
    "    D --> B1\n",
    "    E --> B1\n",
    "    F --> B1\n",
    "    \n",
    "    style A fill:#e1f5ff,color:#333\n",
    "    style B1 fill:#f8d7da,color:#333\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Steps for Responsible AI\n",
    "\n",
    "### As a Developer/Data Scientist\n",
    "\n",
    "1. **Question your assumptions** - Who benefits? Who might be harmed?\n",
    "2. **Diverse teams** - Include varied perspectives in development\n",
    "3. **Representative data** - Ensure training data reflects real-world diversity\n",
    "4. **Test thoroughly** - Evaluate across demographic groups\n",
    "5. **Document everything** - Model cards, data sheets, decision logs\n",
    "6. **Build in oversight** - Human-in-the-loop for critical decisions\n",
    "7. **Plan for failure** - What happens when the model is wrong?\n",
    "8. **Stay informed** - Keep up with ethical AI research and guidelines\n",
    "9. **Speak up** - Raise concerns about problematic uses\n",
    "10. **Iterate responsibly** - Monitor deployed models, fix issues quickly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources for Responsible AI\n",
    "\n",
    "### Tools and Frameworks\n",
    "\n",
    "- **AIF360** (IBM): Fairness metrics and bias mitigation\n",
    "- **Fairlearn** (Microsoft): Fair machine learning toolkit\n",
    "- **What-If Tool** (Google): Visual model exploration\n",
    "- **SHAP**: Model explanation library\n",
    "- **TensorFlow Privacy**: Differential privacy library\n",
    "\n",
    "### Guidelines and Standards\n",
    "\n",
    "- OECD AI Principles\n",
    "- IEEE Ethically Aligned Design\n",
    "- Australia's AI Ethics Principles\n",
    "- Montreal Declaration for Responsible AI\n",
    "- Partnership on AI Best Practices\n",
    "\n",
    "### Educational Resources\n",
    "\n",
    "- [AI Fairness 360](https://aif360.mybluemix.net/)\n",
    "- [Responsible AI Practices (Google)](https://ai.google/responsibilities/responsible-ai-practices/)\n",
    "- [Microsoft Responsible AI Resources](https://www.microsoft.com/en-us/ai/responsible-ai)\n",
    "- [Montreal AI Ethics Institute](https://montrealethics.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study Questions\n",
    "\n",
    "Consider these scenarios:\n",
    "\n",
    "### Scenario 1: Hiring Algorithm\n",
    "A company wants to use an AI system to screen job applications. The model is trained on 10 years of historical hiring data.\n",
    "\n",
    "**Questions to consider:**\n",
    "- What biases might exist in the training data?\n",
    "- How should the model be tested for fairness?\n",
    "- What guardrails should be in place?\n",
    "\n",
    "### Scenario 2: Medical Diagnosis AI\n",
    "A hospital deploys an AI system to help diagnose diseases from medical images.\n",
    "\n",
    "**Questions to consider:**\n",
    "- What if the training data mostly contains images from one demographic?\n",
    "- How should false negatives vs false positives be balanced?\n",
    "- What role should human doctors play?\n",
    "\n",
    "### Scenario 3: Social Media Moderation\n",
    "A platform uses AI to automatically remove harmful content.\n",
    "\n",
    "**Questions to consider:**\n",
    "- How to handle cultural context and different languages?\n",
    "- What about false positives (removing acceptable content)?\n",
    "- How to ensure transparency and appeals process?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
